"""ç­–ç•¥æœåŠ¡å™¨ - è¿è¡Œåœ¨ conda_env_b (Python 3.10)"""
import logging
import os
import sys
import tap
import torch
import time
from pathlib import Path
from typing import Dict, Optional, Any, Tuple

DIFFUSER_HOME = "/home/extra3/yupeng/codespace/updated_3d_diffuser_actor"
sys.path.insert(0, str(DIFFUSER_HOME))
from diffuser_actor.trajectory_optimization.diffuser_actor import DiffuserActor
from diffuser_actor.trajectory_optimization.foresight_diffuser_actor_v3 import ForesightDiffuserActorV3
# from utils.common_utils import get_gripper_loc_bounds
from utils.common_utils_deploy import (
    load_instructions,
    get_gripper_loc_bounds,
    round_floats
)

logging.basicConfig(level=logging.INFO, format='[SERVER] %(message)s')

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
from openpi.serving.ipc_core import BasePolicy, WebsocketPolicyServer, WebsocketRelayServer



class Arguments(tap.Tap):
    """æ¨¡å‹å‚æ•°é…ç½®ç±»"""
    checkpoint: Path = "src/ckpts/epoch_84999.pth"
    seed: int = 2
    device: str = "cuda"
    headless: int = 0
    max_tries: int = 10
    tasks: Tuple[str, ...] = ("pick_moving_target_from_belt",)
    image_size: str = "256,256"
    verbose: int = 0
    max_episodes_per_task: int = 20
    instructions: Optional[Path] = "src/instructions/rmt/rmt_instructions_real_world_dyna5task.pkl"
    variations: Tuple[int, ...] = (-1,)
    cameras: Tuple[str, ...] = ("left_shoulder", "right_shoulder", "wrist")
    accumulate_grad_batches: int = 1
    val_freq: int = 500
    max_steps: int = 25
    test_model: str = "3d_diffuser_actor"
    gripper_loc_bounds_file: str = "/home/extra3/yupeng/codespace/updated_3d_diffuser_actor/tasks/18_peract_tasks_location_bounds.json"
    gripper_loc_bounds_buffer: float = 0.08
    single_task_gripper_loc_bounds: int = 0
    eval_only: int = 0

    # Training and validation datasets
    dense_interpolation: int = 1
    interpolation_length: int = 2

    # Logging to base_log_dir/exp_log_dir/run_log_dir
    base_log_dir: Path = Path(__file__).parent / "train_logs"
    exp_log_dir: str = "exp"
    run_log_dir: str = "run"

    # Main training parameters
    num_workers: int = 1
    batch_size: int = 16
    batch_size_val: int = 4
    cache_size: int = 100
    cache_size_val: int = 100
    lr: float = 1e-4
    wd: float = 5e-3  # used only for CALVIN
    train_iters: int = 200_000
    val_iters: int = -1  # -1 means heuristically-defined
    max_episode_length: int = 5  # -1 for no limit

    # Data augmentations
    image_rescale: str = "0.75,1.25"  # (min, max), "1.0,1.0" for no rescaling

    # Act3D model parameters
    num_query_cross_attn_layers: int = 2
    num_ghost_point_cross_attn_layers: int = 2
    num_ghost_points: int = 10000
    num_ghost_points_val: int = 10000
    weight_tying: int = 1
    gp_emb_tying: int = 1
    num_sampling_level: int = 3
    fine_sampling_ball_diameter: float = 0.16
    regress_position_offset: int = 0

    # 3D Diffuser Actor model parameters
    diffusion_timesteps: int = 100
    num_history: int = 3
    num_future_frames: int = 10
    fps_subsampling_factor: int = 5
    lang_enhanced: int = 0
    dense_interpolation: int = 1
    interpolation_length: int = 2
    relative_action: int = 0
    denoise_model: str = "ddpm"  # "ddpm" or "rectified_flow"
    num_inference_steps: int = 100  # inference steps, RF uses 10

    # Shared model parameters
    action_dim: int = 8
    backbone: str = "clip"  # one of "resnet", "clip"
    embedding_dim: int = 120
    num_vis_ins_attn_layers: int = 2
    use_instruction: int = 1
    rotation_parametrization: str = '6D'
    quaternion_format: str = 'xyzw'



class PolicyServer(BasePolicy):
    """ç­–ç•¥æœåŠ¡å™¨å®ç°"""
    def __init__(self):
        self._model = None
        self._device = None
        self._initialized = False
        self._init_error = None
        self._model_name = None

        args = Arguments().parse_args(args=[]) 
        init_result = self.initialize(args)

        if init_result["status"] == "error":
            # æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼šæ˜¯æŠ›å‡ºå¼‚å¸¸è¿˜æ˜¯ä»…ä»…è®°å½•æ—¥å¿—
            logging.error(f"æ„é€ å‡½æ•°ä¸­çš„è‡ªåŠ¨åˆå§‹åŒ–å¤±è´¥: {init_result['message']}")

    # def initialize(self, model_name: str, model_args: Dict, checkpoint: str, device: str = "cuda") -> Dict:
    #     """åˆå§‹åŒ–æ¨¡å‹
        
    #     Args:
    #         model_name: æ¨¡å‹åç§° ("3d_diffuser_actor" æˆ– "foresight_diffuser_actor_v3")
    #         model_args: æ¨¡å‹å‚æ•°å­—å…¸
    #         checkpoint: checkpoint è·¯å¾„
    #         device: è®¾å¤‡ ("cuda" æˆ– "cpu")
        
    #     Returns:
    #         åŒ…å«åˆå§‹åŒ–çŠ¶æ€çš„å­—å…¸
    #     """
    #     try:
    #         self._device = torch.device(device)
    #         self._model_name = model_name
    #         logging.info(f"åˆå§‹åŒ–æ¨¡å‹: {model_name}, checkpoint: {checkpoint}")
            
    #         # åˆ›å»ºæ¨¡å‹
    #         if model_name == "3d_diffuser_actor":
    #             self._model = DiffuserActor(
    #                 backbone=model_args['backbone'],
    #                 image_size=tuple(model_args['image_size']),
    #                 embedding_dim=model_args['embedding_dim'],
    #                 num_vis_ins_attn_layers=model_args['num_vis_ins_attn_layers'],
    #                 use_instruction=model_args['use_instruction'],
    #                 fps_subsampling_factor=model_args['fps_subsampling_factor'],
    #                 gripper_loc_bounds=model_args['gripper_loc_bounds'],
    #                 rotation_parametrization=model_args['rotation_parametrization'],
    #                 quaternion_format=model_args['quaternion_format'],
    #                 diffusion_timesteps=model_args['diffusion_timesteps'],
    #                 denoise_model=model_args['denoise_model'],
    #                 num_inference_steps=model_args['num_inference_steps'],
    #                 nhist=model_args['nhist'],
    #                 relative=model_args['relative'],
    #                 lang_enhanced=model_args['lang_enhanced'],
    #             ).to(self._device)
    #         elif model_name == "foresight_diffuser_actor_v3":
    #             self._model = ForesightDiffuserActorV3(
    #                 backbone=model_args['backbone'],
    #                 image_size=tuple(model_args['image_size']),
    #                 embedding_dim=model_args['embedding_dim'],
    #                 num_vis_ins_attn_layers=model_args['num_vis_ins_attn_layers'],
    #                 use_instruction=model_args['use_instruction'],
    #                 fps_subsampling_factor=model_args['fps_subsampling_factor'],
    #                 gripper_loc_bounds=model_args['gripper_loc_bounds'],
    #                 rotation_parametrization=model_args['rotation_parametrization'],
    #                 quaternion_format=model_args['quaternion_format'],
    #                 diffusion_timesteps=model_args['diffusion_timesteps'],
    #                 denoise_model=model_args['denoise_model'],
    #                 num_inference_steps=model_args['num_inference_steps'],
    #                 nhist=model_args['nhist'],
    #                 relative=model_args['relative'],
    #                 lang_enhanced=model_args['lang_enhanced'],
    #                 bool_classifier_free_guidance=model_args['bool_classifier_free_guidance'],
    #                 classifier_free_guidance_w=model_args['classifier_free_guidance_w'],
    #                 classifier_free_guidance_dropout_prob=model_args['classifier_free_guidance_dropout_prob'],
    #             ).to(self._device)
    #         else:
    #             raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_name}ï¼Œä»…æ”¯æŒ '3d_diffuser_actor' å’Œ 'foresight_diffuser_actor_v3'")
            
    #         # åŠ è½½ checkpoint
    #         # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
    #         checkpoint_path = Path(checkpoint)
    #         if not checkpoint_path.is_absolute():
    #             checkpoint_path = PROJECT_ROOT / checkpoint_path
            
    #         if checkpoint_path.is_file():
    #             model_dict = torch.load(str(checkpoint_path), map_location="cpu")
    #             model_dict_weight = {}
    #             for key in model_dict["weight"]:
    #                 _key = key[7:]  # ç§»é™¤ "module." å‰ç¼€
    #                 model_dict_weight[_key] = model_dict["weight"][key]
    #             self._model.load_state_dict(model_dict_weight)
    #             logging.info(f"æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ: {checkpoint_path}")
    #         else:
    #             assert False, f"Checkpoint æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}"
            
    #         # è®¾ç½®ä¸º eval æ¨¡å¼
    #         self._model.eval()
    #         self._initialized = True
    #         self._init_error = None
            
    #         logging.info("æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    #         return {
    #             "status": "success",
    #             "message": "æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ",
    #             "model_name": model_name,
    #             "device": str(self._device)
    #         }
    #     except Exception as e:
    #         self._initialized = False
    #         self._init_error = str(e)
    #         logging.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
    #         import traceback
    #         traceback.print_exc()
    #         return {
    #             "status": "error",
    #             "message": f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}",
    #             "error": str(e)
    #         }
    
    def initialize(self, args: Arguments) -> Dict:
        """ä½¿ç”¨ Arguments å¯¹è±¡åˆå§‹åŒ–æ¨¡å‹"""
        try:
            self._device = torch.device(args.device)
            self._model_name = args.test_model
            logging.info(f"åˆå§‹åŒ–æ¨¡å‹: {self._model_name}, checkpoint: {args.checkpoint}")
            
            # å¤„ç† image_size (ä» "256,256" å­—ç¬¦ä¸²è½¬ä¸º tuple)
            img_size = tuple(map(int, args.image_size.split(',')))

            if args.single_task_gripper_loc_bounds and len(args.tasks) == 1:
                task = args.tasks[0]
            else:
                task = None


            gripper_loc_bounds = get_gripper_loc_bounds(
                args.gripper_loc_bounds_file,
                task=task, buffer=args.gripper_loc_bounds_buffer,
            )
            
            # æå–é€šç”¨çš„åŸºç¡€å‚æ•°
            base_kwargs = {
                "backbone": args.backbone,
                "image_size": img_size,
                "embedding_dim": args.embedding_dim,
                "num_vis_ins_attn_layers": args.num_vis_ins_attn_layers,
                "use_instruction": bool(args.use_instruction),
                "fps_subsampling_factor": args.fps_subsampling_factor,
                "gripper_loc_bounds": gripper_loc_bounds, # æ³¨æ„è¿™é‡Œé€šå¸¸ä¼ è·¯å¾„æˆ–é¢„åŠ è½½çš„bounds
                "rotation_parametrization": args.rotation_parametrization,
                "quaternion_format": args.quaternion_format,
                "diffusion_timesteps": args.diffusion_timesteps,
                "denoise_model": args.denoise_model,
                "num_inference_steps": args.num_inference_steps,
                "nhist": args.num_history,
                "relative": bool(args.relative_action),
                "lang_enhanced": bool(args.lang_enhanced),
            }

            # åˆ›å»ºæ¨¡å‹
            if self._model_name == "3d_diffuser_actor":
                self._model = DiffuserActor(**base_kwargs).to(self._device)
                
            elif self._model_name == "foresight_diffuser_actor_v3":
                # æ‰©å±• v3 ç‰¹æœ‰çš„å‚æ•°
                v3_kwargs = {
                    **base_kwargs,
                    "bool_classifier_free_guidance": getattr(args, 'bool_classifier_free_guidance', True),
                    "classifier_free_guidance_w": getattr(args, 'classifier_free_guidance_w', 0.1),
                    "classifier_free_guidance_dropout_prob": getattr(args, 'classifier_free_guidance_dropout_prob', 0.1),
                }
                self._model = ForesightDiffuserActorV3(**v3_kwargs).to(self._device)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {self._model_name}")

            # åŠ è½½ checkpoint (å¤„ç† Path å¯¹è±¡)
            checkpoint_path = Path(args.checkpoint)
            if not checkpoint_path.is_absolute():
                # å‡è®¾ PROJECT_ROOT å·²åœ¨å¤–éƒ¨å®šä¹‰
                checkpoint_path = PROJECT_ROOT / checkpoint_path
            
            if checkpoint_path.is_file():
                model_dict = torch.load(str(checkpoint_path), map_location="cpu")
                # å…¼å®¹ DataParallel çš„ "module." å‰ç¼€
                weights = model_dict.get("weight", model_dict) # å¢åŠ å®¹é”™
                new_weights = {k.replace("module.", ""): v for k, v in weights.items()}
                
                self._model.load_state_dict(new_weights)
                logging.info(f"æˆåŠŸåŠ è½½æƒé‡: {checkpoint_path}")
            else:
                raise FileNotFoundError(f"æœªæ‰¾åˆ° checkpoint: {checkpoint_path}")

            self._model.eval()
            self._initialized = True
            
            return {
                "status": "success",
                "model_name": self._model_name,
                "device": str(self._device)
            }

        except Exception as e:
            self._initialized = False
            logging.error(f"åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            return {"status": "error", "message": str(e)}

    def infer(self, obs: Dict) -> Dict:
        """æ¨ç†æ¥å£
        
        Args:
            obs: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:
                - "fake_traj": torch.Tensor
                - "traj_mask": torch.Tensor
                - "rgbs": torch.Tensor
                - "pcds": torch.Tensor
                - "instr": torch.Tensor
                - "gripper": torch.Tensor
                - "run_inference": bool
                - å…¶ä»– kwargs
        
        Returns:
            åŒ…å« "action" å­—æ®µçš„å­—å…¸
        """
        if not self._initialized:
            raise RuntimeError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        if self._model is None:
            raise RuntimeError("æ¨¡å‹æœªåˆ›å»º")
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        t0 = time.time()
        print("Run inference ... ")
        # æå–å‚æ•°
        fake_traj = obs["fake_traj"]
        traj_mask = obs["traj_mask"]
        rgbs = obs["rgbs"]
        pcds = obs["pcds"]
        instr = obs["instr"]
        gripper = obs["gripper"]
        run_inference = obs.get("run_inference", True)
        
        # æå–å…¶ä»– kwargsï¼ˆåŒ…æ‹¬ next_rgb_obs, next_pcd_obs, next_mask_obs, next_gripper, next_frame_relative_id ç­‰ï¼‰
        kwargs = {k: v for k, v in obs.items() 
                 if k not in ["fake_traj", "traj_mask", "rgbs", "pcds", "instr", "gripper", "run_inference"]}
        
        # ç¡®ä¿å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        fake_traj = fake_traj.to(self._device)
        traj_mask = traj_mask.to(self._device)
        rgbs = rgbs.to(self._device)
        pcds = pcds.to(self._device)
        instr = instr.to(self._device)
        gripper = gripper.to(self._device)
        
        # è½¬æ¢ kwargs ä¸­çš„å¼ é‡
        for k, v in kwargs.items():
            if isinstance(v, torch.Tensor):
                kwargs[k] = v.to(self._device)
            elif v is None:
                # ä¿æŒ None å€¼ä¸å˜ï¼ˆå¦‚ next_mask_obs å¯èƒ½ä¸º Noneï¼‰
                pass
        
        # æ¨ç†
        with torch.no_grad():
            output_dict = self._model(
                fake_traj,
                traj_mask,
                rgbs,
                pcds,
                instr,
                gripper,
                run_inference=run_inference,
                **kwargs
            )
        
        # è¿”å›ç»“æœï¼ˆç¡®ä¿ action åœ¨ CPU ä¸Šä»¥ä¾¿åºåˆ—åŒ–ï¼‰
        result = {
            "action": output_dict["action"].cpu() if isinstance(output_dict["action"], torch.Tensor) else output_dict["action"]
        }
        
        # å¦‚æœ output_dict ä¸­æœ‰å…¶ä»–å­—æ®µï¼Œä¹Ÿè¿”å›
        for k, v in output_dict.items():
            if k != "action":
                if isinstance(v, torch.Tensor):
                    result[k] = v.cpu()
                else:
                    result[k] = v
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        print(f"[SERVER] infer: {(time.time() - t0)*1000:.2f}ms")
        print("Inference done ... ")
        return result

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--host", type=str, default="0.0.0.0", help="æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--port", type=int, default=8766, help="æœåŠ¡å™¨ç«¯å£")
    args = parser.parse_args()
    
    policy = PolicyServer()
    server = WebsocketRelayServer(policy, host=args.host, port=args.port, 
                                  target_host="127.0.0.1", target_port=8767)
    print("ğŸš€ ç­–ç•¥æœåŠ¡å™¨è¿è¡Œä¸­...")
    server.serve_forever()
