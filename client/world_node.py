"""WorldModel æœåŠ¡å™¨ - è¿è¡Œåœ¨ conda_env_b (Python 3.10)"""
import logging
import os
import sys
import torch
import time
from pathlib import Path
from typing import Dict, Optional
DIFFUSER_HOME = "/home/extra3/yupeng/codespace/updated_3d_diffuser_actor"
sys.path.insert(0, str(DIFFUSER_HOME))

logging.basicConfig(level=logging.INFO, format='[WORLD_SERVER] %(message)s')

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
from robot_tool_box.config_loader import get_server_config, get_camera_config
from openpi.serving.ipc_core import BasePolicy, WebsocketPolicyServer, WebsocketRelayServer


class DummyWorldModelServer(BasePolicy):
    """WorldModel è°ƒè¯•ä¼ªç±»ï¼šè¿”å›å›ºå®šæ ¼å¼çš„ Mock æ•°æ®"""
    def __init__(self):
        self._initialized = False
        self._device = "cpu"

        self._cameras_config = get_camera_config()
        
        logging.info("DummyWorldModelServer å·²åˆ›å»º (è°ƒè¯•æ¨¡å¼)")

    def initialize(self, **kwargs) -> Dict:
        """æ¨¡æ‹Ÿåˆå§‹åŒ–ï¼Œä¿å­˜å‚æ•°å¹¶è¿”å›æˆåŠŸ"""
        self._initialized = True
        logging.info("DummyWorldModelServer åˆå§‹åŒ–æˆåŠŸ")
        return {
            "status": "success",
            "message": "DummyWorldModel é…ç½®åˆå§‹åŒ–æˆåŠŸ",
            "device": "cpu"
        }

    def infer(self, request: Dict) -> Dict:
        """æ¨¡æ‹Ÿæ¨ç†è·¯ç”±"""
        if not self._initialized:
            raise RuntimeError("WorldModel é…ç½®æœªåˆå§‹åŒ–")
        
        request_type = request.get("type")
        # æ¨¡æ‹Ÿè®¡ç®—å»¶è¿Ÿ
        time.sleep(0.05) 
        
        if request_type == "init":
            return self._handle_init(request)
        elif request_type == "update":
            return self._handle_update(request)
        elif request_type == "predict":
            return self._handle_predict(request)
        else:
            raise ValueError(f"æœªçŸ¥çš„è¯·æ±‚ç±»å‹: {request_type}")

    def _handle_init(self, request: Dict) -> Dict:
        """è¿”å›å›ºå®šçš„ init ç»“æœ"""
        logging.info("[Dummy] å¤„ç† init è¯·æ±‚")
        # æ¨¡æ‹Ÿä¸€ä¸ª 1x10 çš„ fake tensor ç»“æ„
        fake_network_t = np.zeros((1, 10), dtype=np.float32)
        
        return {
            "status": "success",
            "message": "Dummy WorldModel init æˆåŠŸ",
            "network_t": fake_network_t,
        }

    def _handle_update(self, request: Dict) -> Dict:
        """è¿”å›å›ºå®šçš„ update ç»“æœ"""
        logging.info(f"[Dummy] å¤„ç† update è¯·æ±‚: cur_t={request.get('cur_t')}")
        fake_network_t = np.zeros((1, 10), dtype=np.float32)
        
        return {
            "status": "success",
            "message": "Dummy WorldModel update æˆåŠŸ",
            "network_t": fake_network_t,
        }

    def _handle_predict(self, request: Dict) -> Dict:
        """è¿”å›å›ºå®šé•¿åº¦çš„åˆ—è¡¨å’Œæ•°æ®"""
        num_frames = request.get("num_future_frames", 30)
        logging.info(f"[Dummy] å¤„ç† predict è¯·æ±‚: num_frames={num_frames}")
        
        # æ„é€ å›ºå®šç»´åº¦çš„æ¨¡æ‹Ÿå›¾åƒå’Œæ·±åº¦æ•°æ® (å‡è®¾ 128x128)
        fake_img = np.random.randint(0, 255, (3, 128, 128), dtype=np.uint8)
        fake_depth = np.random.rand(1, 128, 128).astype(np.float32)
        fake_pose = np.array([0, 0, 0, 0, 0, 0, 1], dtype=np.float32) # [pos, quat]

        return {
            "status": "success",
            "image_list": [fake_img for _ in range(num_frames)],
            "depth_list": [fake_depth for _ in range(num_frames)],
            "semantic_mask_list": None,
            "target_time_list": [float(i) for i in range(num_frames)],
            "eepose_list": [fake_pose for _ in range(num_frames)],
            "openness_list": [1.0 for _ in range(num_frames)],
        }

    def infer(self, request: Dict) -> Dict:
        """å¤„ç†æ¨ç†è¯·æ±‚
        
        Args:
            request: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:
                - "type": "init" | "update" | "predict"
                - å…¶ä»–å­—æ®µæ ¹æ®ç±»å‹ä¸åŒè€Œä¸åŒ
        
        Returns:
            åŒ…å«ç»“æœçš„å­—å…¸
        """
        if not self._initialized:
            raise RuntimeError("WorldModel é…ç½®æœªåˆå§‹åŒ–")
        
        request_type = request.get("type")
        
        if request_type == "init":
            return self._handle_init(request)
        elif request_type == "update":
            return self._handle_update(request)
        elif request_type == "predict":
            return self._handle_predict(request)
        else:
            raise ValueError(f"æœªçŸ¥çš„è¯·æ±‚ç±»å‹: {request_type}")


if __name__ == "__main__":
    import argparse
    #disable_deterministic_algorithms()    
    parser = argparse.ArgumentParser()
    parser.add_argument("--host", type=str, default="0.0.0.0", help="æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--port", type=int, default=8765, help="æœåŠ¡å™¨ç«¯å£")
    args = parser.parse_args()
    os.environ.pop('CUBLAS_WORKSPACE_CONFIG', None)
    torch.backends.cudnn.benchmark = True
    torch.use_deterministic_algorithms(False)
    torch.set_float32_matmul_precision('medium')
    world_model_server = DummyWorldModelServer()
    server = WebsocketRelayServer(world_model_server, host=args.host, port=args.port,
                                    target_host="127.0.0.1", target_port=8766)
    print("ğŸš€ WorldModel æœåŠ¡å™¨è¿è¡Œä¸­...")
    server.serve_forever()
